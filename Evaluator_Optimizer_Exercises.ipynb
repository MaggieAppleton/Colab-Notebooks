{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaggieAppleton/Colab-Notebooks/blob/main/Evaluator_Optimizer_Exercises.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PefAqrDeBdVK"
      },
      "source": [
        "# Evaluator-Optimizer Workflow Learning Exercises\n",
        "\n",
        "This notebook helps you master the **Evaluator-Optimizer** pattern through progressive exercises.\n",
        "\n",
        "## The Pattern\n",
        "In the evaluator-optimizer workflow:\n",
        "1. **Generator LLM** creates content or solutions\n",
        "2. **Evaluator LLM** assesses quality and provides feedback\n",
        "3. **Loop continues** until the evaluator is satisfied\n",
        "\n",
        "## When to Use This Pattern\n",
        "- When you have clear evaluation criteria\n",
        "- When iterative refinement provides measurable value\n",
        "- When LLM responses can be demonstrably improved with feedback\n",
        "- Examples: literary translation, content writing, code optimization\n",
        "\n",
        "## Setup\n",
        "First, let's install the required packages and set up our API client."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2EHbm_yBdVL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f43d8628-095d-4208-e7db-8098eb205b67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting anthropic\n",
            "  Downloading anthropic-0.57.1-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from anthropic) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from anthropic) (4.14.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->anthropic) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->anthropic) (2025.7.9)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->anthropic) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.4.1)\n",
            "Downloading anthropic-0.57.1-py3-none-any.whl (292 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.8/292.8 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: anthropic\n",
            "Successfully installed anthropic-0.57.1\n"
          ]
        }
      ],
      "source": [
        "!pip install anthropic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yy2T74AZBdVL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25b8772a-281e-4c68-c5b3-a797cea120ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete!\n"
          ]
        }
      ],
      "source": [
        "import anthropic\n",
        "import re\n",
        "from google.colab import userdata\n",
        "\n",
        "# Initialize the Anthropic client\n",
        "anthropic_api_key = userdata.get('ANTHROPIC_API_KEY')\n",
        "client = anthropic.Anthropic(api_key=anthropic_api_key)\n",
        "\n",
        "def call_claude(prompt, model=\"claude-3-5-haiku-latest\"):\n",
        "    \"\"\"Make a call to the Claude API\"\"\"\n",
        "    response = client.messages.create(\n",
        "        model=model,\n",
        "        max_tokens=500,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "    return response.content[0].text\n",
        "\n",
        "def extract_xml(text, tag):\n",
        "    \"\"\"Extract content from XML tags\"\"\"\n",
        "    pattern = f'<{tag}>(.*?)</{tag}>'\n",
        "    match = re.search(pattern, text, re.DOTALL)\n",
        "    return match.group(1).strip() if match else \"\"\n",
        "\n",
        "print(\"Setup complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhT8sVMbBdVM"
      },
      "source": [
        "# Exercise 1: Understanding the Components (Warmup)\n",
        "\n",
        "Before building the full workflow, let's understand each component.\n",
        "\n",
        "## Exercise 1.1: Build a Simple Generator\n",
        "\n",
        "**Your task**: Create a function that generates content based on a task description.\n",
        "\n",
        "**Hints**:\n",
        "- The generator should take a task and return generated content\n",
        "- Use XML tags to structure the LLM's response\n",
        "- Think about what makes a good prompt for content generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__TqwlsPBdVM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8197fdfb-aef2-4c47-af94-7b093708a613"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: You are an expert copywriter. Complete this task: Write a short product description for eco-friendly bamboo toothbrushes. Take this additional information into consideration while you complete it: Previous versions of this were too hokey and inauthentic.\n",
            " Wrap your response in <content> tags.\n",
            "\n",
            "Generated content:\n",
            "Ditch plastic, embrace sustainability. Our bamboo toothbrushes are minimalist, functional, and kind to the planet. Crafted from 100% biodegradable bamboo with soft, BPA-free bristles, these brushes reduce landfill waste without compromising on cleaning performance. Each brush is ergonomically designed with a comfortable grip and naturally antimicrobial bamboo handle. Simple, effective, and environmentally responsible—because taking care of your teeth shouldn't cost the earth.\n"
          ]
        }
      ],
      "source": [
        "def generate_content(task, context=None):\n",
        "    \"\"\"Generate content based on a task description.\n",
        "\n",
        "    Args:\n",
        "        task: String describing what to generate\n",
        "        context: Optional context like previous attempts or feedback\n",
        "\n",
        "    Returns:\n",
        "        Generated content as a string\n",
        "    \"\"\"\n",
        "    # TODO: Create a prompt that asks the LLM to generate content\n",
        "    # - Include the task description\n",
        "    # - If context is provided, include it to help improve the generation\n",
        "    # - Ask the LLM to wrap its response in <content></content> tags\n",
        "\n",
        "    context_prompt = \"\"\n",
        "    if context:\n",
        "      context_prompt += f\"Take this additional information into consideration while you complete it: {context}\\n\"\n",
        "\n",
        "    prompt = f\"You are an expert copywriter. Complete this task: {task}. {context_prompt} Wrap your response in <content> tags.\"\n",
        "\n",
        "    print(f\"Prompt: {prompt}\\n\")\n",
        "\n",
        "    # TODO: Call the LLM and extract the content\n",
        "    response = call_claude(prompt)\n",
        "    content = extract_xml(response, 'content')\n",
        "\n",
        "    return content\n",
        "\n",
        "# Test your generator\n",
        "test_task = \"Write a short product description for eco-friendly bamboo toothbrushes\"\n",
        "test_context = \"Previous versions of this were too hokey and inauthentic.\"\n",
        "result = generate_content(test_task, test_context)\n",
        "print(\"Generated content:\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyRP_Qu7BdVM"
      },
      "source": [
        "## Exercise 1.2: Build a Simple Evaluator\n",
        "\n",
        "**Your task**: Create a function that evaluates content quality.\n",
        "\n",
        "**Hints**:\n",
        "- The evaluator should return both a pass/fail decision and specific feedback\n",
        "- Think about what criteria make content good for the given task\n",
        "- Use XML tags to structure the evaluation response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMScxW8fBdVM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d449d845-6338-4e4d-85af-4d4f023e42e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: \n",
            "You are an expert copywriter. Another copywriter was given this task: Write a short product description for eco-friendly bamboo toothbrushes.\n",
            "Evaluate their content based on this criteria: clarity, completeness, and originality. Return your evaluation inside <evaluation> tags.\n",
            "Decide whether this content should get a PASS score or a NEEDS_IMPROVEMENT score, along with specific feedback about why you made that decision. Wrap your score inside <score> tags and your feedback inside <feedback> tags.\n",
            "\n",
            "Content: Ditch plastic, embrace sustainability. Our bamboo toothbrushes are minimalist, functional, and kind to the planet. Crafted from 100% biodegradable bamboo with soft, BPA-free bristles, these brushes reduce landfill waste without compromising on cleaning performance. Each brush is ergonomically designed with a comfortable grip and naturally antimicrobial bamboo handle. Simple, effective, and environmentally responsible—because taking care of your teeth shouldn't cost the earth.\n",
            "    \n",
            "---- Evaluation: ----\n",
            "Clarity: Very clear, concise messaging that communicates key product benefits\n",
            "Completeness: Covers materials, environmental impact, design features, and performance\n",
            "Originality: Strong, engaging language with a compelling sustainability angle\n",
            "\n",
            "---- Score: ----\n",
            "PASS\n",
            "\n",
            "---- Feedback: ----\n",
            "This copy effectively:\n",
            "- Uses strong, active language (\"Ditch plastic\")\n",
            "- Highlights multiple product benefits (eco-friendly, functional, ergonomic)\n",
            "- Connects product features to broader environmental values\n",
            "- Includes specific technical details (100% biodegradable, BPA-free, antimicrobial)\n",
            "- Ends with a memorable, impactful tagline that reinforces the sustainability message\n",
            "\n",
            "The only potential improvement might be including specific details about the brush's size or packaging, but overall the description is excellent and meets all core copywriting objectives.\n"
          ]
        }
      ],
      "source": [
        "def evaluate_content(content, task, criteria=None):\n",
        "    \"\"\"Evaluate content quality against task requirements.\n",
        "\n",
        "    Args:\n",
        "        content: The generated content to evaluate\n",
        "        task: Original task description\n",
        "        criteria: Optional specific criteria to check\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (evaluation_result, feedback)\n",
        "        evaluation_result: \"PASS\" or \"NEEDS_IMPROVEMENT\"\n",
        "        feedback: Specific suggestions for improvement\n",
        "    \"\"\"\n",
        "    # TODO: Create a prompt that asks the LLM to evaluate the content\n",
        "    # - Include the original task and the content to evaluate\n",
        "    # - Ask for specific evaluation criteria (clarity, completeness, etc.)\n",
        "    # - Request both a PASS/NEEDS_IMPROVEMENT decision and specific feedback\n",
        "    # - Use XML tags like <evaluation></evaluation> and <feedback></feedback>\n",
        "\n",
        "    criteria_prompt = \"\"\n",
        "    if criteria:\n",
        "      criteria_prompt += f\"Evaluate their content based on this criteria: {criteria}. Return your evaluation inside <evaluation> tags.\"\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are an expert copywriter. Another copywriter was given this task: {task}.\n",
        "{criteria_prompt}\n",
        "Decide whether this content should get a PASS score or a NEEDS_IMPROVEMENT score, along with specific feedback about why you made that decision. Wrap your score inside <score> tags and your feedback inside <feedback> tags.\n",
        "\n",
        "Content: {content}\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "\n",
        "    response = call_claude(prompt)\n",
        "    evaluation = extract_xml(response, 'evaluation')\n",
        "    feedback = extract_xml(response, 'feedback')\n",
        "    score = extract_xml(response, 'score')\n",
        "\n",
        "    return evaluation, feedback, score\n",
        "\n",
        "test_task = \"Write a short product description for eco-friendly bamboo toothbrushes\"\n",
        "test_content = \"Ditch plastic, embrace sustainability. Our bamboo toothbrushes are minimalist, functional, and kind to the planet. Crafted from 100% biodegradable bamboo with soft, BPA-free bristles, these brushes reduce landfill waste without compromising on cleaning performance. Each brush is ergonomically designed with a comfortable grip and naturally antimicrobial bamboo handle. Simple, effective, and environmentally responsible—because taking care of your teeth shouldn't cost the earth.\"\n",
        "test_criteria = \"clarity, completeness, and originality\"\n",
        "\n",
        "evaluation, feedback, score = evaluate_content(test_content, test_task, test_criteria)\n",
        "\n",
        "print(f\"---- Evaluation: ----\\n{evaluation}\\n\")\n",
        "print(f\"---- Score: ----\\n{score}\\n\")\n",
        "print(f\"---- Feedback: ----\\n{feedback}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYpdky8yBdVM"
      },
      "source": [
        "# Exercise 2: Basic Evaluator-Optimizer Loop (Core Pattern)\n",
        "\n",
        "Now let's combine the generator and evaluator into the full workflow pattern.\n",
        "\n",
        "**Your task**: Build a complete evaluator-optimizer loop that iterates until the content passes evaluation.\n",
        "\n",
        "**Key concepts to implement**:\n",
        "- Iteration loop with maximum attempts\n",
        "- Passing feedback from evaluator back to generator\n",
        "- Stopping when content passes evaluation\n",
        "- Tracking attempts for context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6eipSnLDBdVM"
      },
      "outputs": [],
      "source": [
        "def evaluator_optimizer_loop(task, max_iterations=3):\n",
        "    \"\"\"Run the complete evaluator-optimizer workflow.\n",
        "\n",
        "    Args:\n",
        "        task: The task description\n",
        "        max_iterations: Maximum number of improvement attempts\n",
        "\n",
        "    Returns:\n",
        "        Final content (whether it passed or not)\n",
        "    \"\"\"\n",
        "    attempts = []  # Track all attempts\n",
        "    feedback_history = []  # Track feedback\n",
        "\n",
        "    for iteration in range(max_iterations):\n",
        "        print(f\"\\n=== ITERATION {iteration + 1} ===\")\n",
        "\n",
        "        # TODO: Generate content\n",
        "        # - On first iteration, just use the task\n",
        "        # - On later iterations, include previous attempts and feedback as context\n",
        "\n",
        "        if iteration == 0:\n",
        "            # First attempt - no context needed\n",
        "            content = \"YOUR CODE HERE\"\n",
        "        else:\n",
        "            # Build context from previous attempts and feedback\n",
        "            context = \"YOUR CODE HERE (combine attempts and feedback)\"\n",
        "            content = \"YOUR CODE HERE\"\n",
        "\n",
        "        print(f\"Generated content: {content}\")\n",
        "        attempts.append(content)\n",
        "\n",
        "        # TODO: Evaluate the content\n",
        "        evaluation, feedback = \"YOUR CODE HERE\"\n",
        "\n",
        "        print(f\"Evaluation: {evaluation}\")\n",
        "        print(f\"Feedback: {feedback}\")\n",
        "\n",
        "        feedback_history.append(feedback)\n",
        "\n",
        "        # TODO: Check if we should stop iterating\n",
        "        if \"YOUR CODE HERE (check if evaluation passed)\":\n",
        "            print(\"\\n✅ Content approved!\")\n",
        "            return content\n",
        "\n",
        "    print(\"\\n⚠️ Max iterations reached without approval\")\n",
        "    return attempts[-1]  # Return the last attempt\n",
        "\n",
        "# Test the complete workflow\n",
        "task = \"Write an email to a customer explaining a delayed shipment, showing empathy and offering a solution\"\n",
        "final_content = evaluator_optimizer_loop(task)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"FINAL RESULT:\")\n",
        "print(final_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICxvzTCbBdVN"
      },
      "source": [
        "# Exercise 3: Multi-Criteria Evaluation (Intermediate)\n",
        "\n",
        "Real-world content often needs to meet multiple criteria. Let's build an evaluator that checks different aspects separately.\n",
        "\n",
        "**Your task**: Create an evaluator that checks multiple criteria and only passes if ALL criteria are met.\n",
        "\n",
        "**Concepts to learn**:\n",
        "- Breaking evaluation into separate criteria\n",
        "- Combining multiple evaluation results\n",
        "- Providing targeted feedback for each criterion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24uWmugLBdVN"
      },
      "outputs": [],
      "source": [
        "def evaluate_multiple_criteria(content, task, criteria_dict):\n",
        "    \"\"\"Evaluate content against multiple criteria separately.\n",
        "\n",
        "    Args:\n",
        "        content: Content to evaluate\n",
        "        task: Original task\n",
        "        criteria_dict: Dict of {criterion_name: criterion_description}\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (overall_result, detailed_feedback)\n",
        "    \"\"\"\n",
        "    criterion_results = {}\n",
        "\n",
        "    # TODO: Evaluate each criterion separately\n",
        "    for criterion_name, criterion_description in criteria_dict.items():\n",
        "        print(f\"  Checking {criterion_name}...\")\n",
        "\n",
        "        # TODO: Create a prompt focused on this specific criterion\n",
        "        # Include the criterion description and ask for pass/fail + feedback\n",
        "\n",
        "        prompt = \"YOUR CODE HERE\"\n",
        "\n",
        "        # TODO: Get evaluation for this criterion\n",
        "        response = \"YOUR CODE HERE\"\n",
        "        evaluation = \"YOUR CODE HERE\"\n",
        "        feedback = \"YOUR CODE HERE\"\n",
        "\n",
        "        criterion_results[criterion_name] = {\n",
        "            \"evaluation\": evaluation,\n",
        "            \"feedback\": feedback\n",
        "        }\n",
        "\n",
        "        print(f\"    {criterion_name}: {evaluation}\")\n",
        "        if feedback:\n",
        "            print(f\"    Feedback: {feedback}\")\n",
        "\n",
        "    # TODO: Determine overall result\n",
        "    # Should pass only if ALL criteria pass\n",
        "    all_passed = \"YOUR CODE HERE\"\n",
        "\n",
        "    # TODO: Compile feedback from failed criteria\n",
        "    failed_feedback = []\n",
        "    for criterion_name, result in criterion_results.items():\n",
        "        if \"YOUR CODE HERE (check if this criterion failed)\":\n",
        "            failed_feedback.append(f\"{criterion_name}: {result['feedback']}\")\n",
        "\n",
        "    overall_feedback = \"\\n\".join(failed_feedback) if failed_feedback else \"All criteria met!\"\n",
        "    overall_result = \"PASS\" if all_passed else \"NEEDS_IMPROVEMENT\"\n",
        "\n",
        "    return overall_result, overall_feedback\n",
        "\n",
        "def multi_criteria_optimizer(task, criteria_dict, max_iterations=4):\n",
        "    \"\"\"Run evaluator-optimizer with multiple criteria.\"\"\"\n",
        "    attempts = []\n",
        "\n",
        "    for iteration in range(max_iterations):\n",
        "        print(f\"\\n=== ITERATION {iteration + 1} ===\")\n",
        "\n",
        "        # TODO: Generate content (similar to Exercise 2)\n",
        "        if iteration == 0:\n",
        "            content = generate_content(task)\n",
        "        else:\n",
        "            # Build context from previous attempts\n",
        "            context = \"YOUR CODE HERE\"\n",
        "            content = generate_content(task, context)\n",
        "\n",
        "        print(f\"Generated: {content}\")\n",
        "        attempts.append(content)\n",
        "\n",
        "        # TODO: Evaluate against all criteria\n",
        "        print(\"\\nEvaluating criteria:\")\n",
        "        evaluation, feedback = \"YOUR CODE HERE\"\n",
        "\n",
        "        print(f\"\\nOverall evaluation: {evaluation}\")\n",
        "        print(f\"Feedback: {feedback}\")\n",
        "\n",
        "        # TODO: Check if we should stop\n",
        "        if \"YOUR CODE HERE\":\n",
        "            print(\"\\n✅ All criteria satisfied!\")\n",
        "            return content\n",
        "\n",
        "    print(\"\\n⚠️ Max iterations reached\")\n",
        "    return attempts[-1]\n",
        "\n",
        "# Test with multiple criteria\n",
        "task = \"Write a blog post introduction about the benefits of remote work\"\n",
        "\n",
        "criteria = {\n",
        "    \"engagement\": \"Check if the introduction is engaging and hooks the reader\",\n",
        "    \"clarity\": \"Evaluate if the writing is clear and easy to understand\",\n",
        "    \"relevance\": \"Assess if the content directly addresses remote work benefits\",\n",
        "    \"structure\": \"Check if the introduction has good flow and structure\"\n",
        "}\n",
        "\n",
        "result = multi_criteria_optimizer(task, criteria)\n",
        "print(f\"\\nFinal result: {result}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5CgySAgBdVN"
      },
      "source": [
        "# Exercise 4: Code Quality Optimizer (Advanced)\n",
        "\n",
        "Let's apply the evaluator-optimizer pattern to code generation and improvement.\n",
        "\n",
        "**Your task**: Build a system that generates code, evaluates it for both functionality and quality, and iteratively improves it.\n",
        "\n",
        "**Advanced concepts**:\n",
        "- Domain-specific evaluation (code functionality vs. code quality)\n",
        "- Handling more complex context and feedback\n",
        "- Multiple evaluation dimensions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Px5GpmDBdVN"
      },
      "outputs": [],
      "source": [
        "def generate_code(task, requirements=None, context=None):\n",
        "    \"\"\"Generate code based on task description.\n",
        "\n",
        "    Args:\n",
        "        task: Description of what the code should do\n",
        "        requirements: Specific technical requirements\n",
        "        context: Previous attempts and feedback\n",
        "\n",
        "    Returns:\n",
        "        Generated code as string\n",
        "    \"\"\"\n",
        "    # TODO: Create a prompt for code generation\n",
        "    # - Include the task description\n",
        "    # - Include any specific requirements\n",
        "    # - If context provided, include previous attempts and feedback\n",
        "    # - Ask for complete, working code wrapped in <code></code> tags\n",
        "    # - Emphasize good practices like comments, error handling, etc.\n",
        "\n",
        "    prompt = \"YOUR CODE HERE\"\n",
        "\n",
        "    response = llm_call(prompt)\n",
        "    code = extract_xml(response, \"code\")\n",
        "\n",
        "    return code\n",
        "\n",
        "def evaluate_code_functionality(code, task, test_cases=None):\n",
        "    \"\"\"Evaluate if code meets functional requirements.\"\"\"\n",
        "    # TODO: Create a prompt to evaluate code functionality\n",
        "    # - Include the original task\n",
        "    # - Include the code to evaluate\n",
        "    # - If test_cases provided, ask to consider them\n",
        "    # - Focus on correctness, handling edge cases, etc.\n",
        "\n",
        "    prompt = \"YOUR CODE HERE\"\n",
        "\n",
        "    response = llm_call(prompt)\n",
        "    evaluation = extract_xml(response, \"evaluation\")\n",
        "    feedback = extract_xml(response, \"feedback\")\n",
        "\n",
        "    return evaluation, feedback\n",
        "\n",
        "def evaluate_code_quality(code):\n",
        "    \"\"\"Evaluate code quality and best practices.\"\"\"\n",
        "    # TODO: Create a prompt to evaluate code quality\n",
        "    # - Focus on readability, maintainability, efficiency\n",
        "    # - Check for proper variable names, comments, error handling\n",
        "    # - Look for code smells or anti-patterns\n",
        "\n",
        "    prompt = \"YOUR CODE HERE\"\n",
        "\n",
        "    response = llm_call(prompt)\n",
        "    evaluation = extract_xml(response, \"evaluation\")\n",
        "    feedback = extract_xml(response, \"feedback\")\n",
        "\n",
        "    return evaluation, feedback\n",
        "\n",
        "def code_evaluator_optimizer(task, requirements=None, test_cases=None, max_iterations=4):\n",
        "    \"\"\"Generate and iteratively improve code.\"\"\"\n",
        "    attempts = []\n",
        "\n",
        "    for iteration in range(max_iterations):\n",
        "        print(f\"\\n=== ITERATION {iteration + 1} ===\")\n",
        "\n",
        "        # TODO: Generate code\n",
        "        if iteration == 0:\n",
        "            code = \"YOUR CODE HERE\"\n",
        "        else:\n",
        "            # Build context from previous attempts and feedback\n",
        "            context = \"YOUR CODE HERE\"\n",
        "            code = \"YOUR CODE HERE\"\n",
        "\n",
        "        print(f\"Generated code:\\n{code}\")\n",
        "        attempts.append(code)\n",
        "\n",
        "        # TODO: Evaluate both functionality and quality\n",
        "        print(\"\\nEvaluating functionality:\")\n",
        "        func_eval, func_feedback = \"YOUR CODE HERE\"\n",
        "\n",
        "        print(f\"  Functionality: {func_eval}\")\n",
        "        if func_feedback:\n",
        "            print(f\"  Feedback: {func_feedback}\")\n",
        "\n",
        "        print(\"\\nEvaluating quality:\")\n",
        "        quality_eval, quality_feedback = \"YOUR CODE HERE\"\n",
        "\n",
        "        print(f\"  Quality: {quality_eval}\")\n",
        "        if quality_feedback:\n",
        "            print(f\"  Feedback: {quality_feedback}\")\n",
        "\n",
        "        # TODO: Determine if both aspects pass\n",
        "        both_pass = \"YOUR CODE HERE\"\n",
        "\n",
        "        # TODO: Combine feedback for next iteration\n",
        "        combined_feedback = \"YOUR CODE HERE\"\n",
        "\n",
        "        if both_pass:\n",
        "            print(\"\\n✅ Code passes both functionality and quality checks!\")\n",
        "            return code\n",
        "\n",
        "    print(\"\\n⚠️ Max iterations reached\")\n",
        "    return attempts[-1]\n",
        "\n",
        "# Test the code optimizer\n",
        "task = \"Create a function that calculates the factorial of a number\"\n",
        "requirements = [\n",
        "    \"Handle edge cases (0, negative numbers)\",\n",
        "    \"Include proper error handling\",\n",
        "    \"Add docstring and comments\",\n",
        "    \"Use efficient approach\"\n",
        "]\n",
        "\n",
        "test_cases = [\n",
        "    \"factorial(5) should return 120\",\n",
        "    \"factorial(0) should return 1\",\n",
        "    \"factorial(-1) should handle error gracefully\"\n",
        "]\n",
        "\n",
        "final_code = code_evaluator_optimizer(task, requirements, test_cases)\n",
        "print(f\"\\nFinal code:\\n{final_code}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDZ2yLgqBdVN"
      },
      "source": [
        "# Exercise 5: Creative Challenge (Open-ended)\n",
        "\n",
        "Now that you understand the pattern, apply it to a domain of your choice!\n",
        "\n",
        "**Your task**: Pick a creative application and implement an evaluator-optimizer workflow for it.\n",
        "\n",
        "**Ideas to consider**:\n",
        "- Recipe optimization (taste, nutrition, cost)\n",
        "- Marketing copy improvement (persuasiveness, clarity, brand voice)\n",
        "- Story writing (plot, character development, pacing)\n",
        "- Technical documentation (accuracy, completeness, readability)\n",
        "- UI/UX copy (clarity, helpfulness, tone)\n",
        "\n",
        "**Requirements**:\n",
        "- Define your own evaluation criteria\n",
        "- Implement the full workflow\n",
        "- Test with a realistic example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gxQXaXlBdVO"
      },
      "outputs": [],
      "source": [
        "# YOUR CREATIVE IMPLEMENTATION HERE\n",
        "# Choose your domain and implement a custom evaluator-optimizer workflow\n",
        "\n",
        "def my_custom_generator(task, context=None):\n",
        "    \"\"\"Generate content for your chosen domain.\"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    pass\n",
        "\n",
        "def my_custom_evaluator(content, task, criteria):\n",
        "    \"\"\"Evaluate content according to your domain's requirements.\"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    pass\n",
        "\n",
        "def my_custom_optimizer(task, criteria, max_iterations=3):\n",
        "    \"\"\"Full workflow for your chosen domain.\"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    pass\n",
        "\n",
        "# Test your implementation\n",
        "# my_task = \"...\"\n",
        "# my_criteria = {...}\n",
        "# result = my_custom_optimizer(my_task, my_criteria)\n",
        "# print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8E9n6j5ABdVO"
      },
      "source": [
        "# Summary and Key Takeaways\n",
        "\n",
        "Congratulations! You've now learned the **Evaluator-Optimizer** pattern through hands-on practice. Here are the key concepts you've mastered:\n",
        "\n",
        "## Core Pattern Components\n",
        "1. **Generator**: Creates content based on task description and context\n",
        "2. **Evaluator**: Assesses quality and provides specific feedback\n",
        "3. **Loop**: Iterates until content meets standards or max attempts reached\n",
        "4. **Context**: Previous attempts and feedback inform future generations\n",
        "\n",
        "## When to Use This Pattern\n",
        "- ✅ **Good for**: Content with measurable quality criteria\n",
        "- ✅ **Good for**: Tasks where iteration provides clear value\n",
        "- ✅ **Good for**: Complex outputs requiring multiple criteria\n",
        "- ❌ **Avoid for**: Simple factual queries or single-shot tasks\n",
        "- ❌ **Avoid for**: Real-time applications (due to multiple LLM calls)\n",
        "\n",
        "## Best Practices Learned\n",
        "- Use XML tags for structured LLM responses\n",
        "- Provide specific, actionable feedback\n",
        "- Build rich context from previous attempts\n",
        "- Set reasonable iteration limits\n",
        "- Evaluate multiple criteria separately when needed\n",
        "- Combine functional and qualitative evaluations for code\n",
        "\n",
        "## Next Steps\n",
        "- Try applying this pattern to your own use cases\n",
        "- Experiment with different evaluation criteria\n",
        "- Consider combining with other AI engineering patterns\n",
        "- Optimize for your specific domain requirements\n",
        "\n",
        "## Resources for Further Learning\n",
        "- Anthropic's prompt engineering documentation\n",
        "- Advanced prompt patterns and techniques\n",
        "- Production considerations for multi-step workflows\n",
        "\n",
        "Happy building! 🚀"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}